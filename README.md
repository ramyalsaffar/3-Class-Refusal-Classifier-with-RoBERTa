# 3-Class Refusal Classifier with RoBERTa

A production-ready, fine-tuned RoBERTa model for detecting refusal patterns in Large Language Model (LLM) responses with **dual-task classification**:
1. **Refusal Classification** (3 classes): No Refusal, Hard Refusal, Soft Refusal
2. **Jailbreak Detection** (2 classes): Attack Failed, Attack Succeeded

---

## ğŸ¯ Features

### **Core Capabilities:**
- âœ… **Dual-Task Classification**: Simultaneously detects refusal patterns AND jailbreak success
- âœ… **Production-Ready**: Full API server with A/B testing, monitoring, and auto-retraining
- âœ… **Comprehensive Interpretability**: SHAP analysis, attention visualization, power law analysis
- âœ… **LLM Judge Labeling**: GPT-4-based labeling with confidence scores and randomized class ordering
- âœ… **3-Stage Prompt Generation**: Quality-controlled prompt generation pipeline
- âœ… **Multi-Model Data Collection**: Claude Sonnet 4.5, GPT-5, Gemini 2.5 Flash
- âœ… **Generic Design**: Works with any N-class classifier (not hardcoded to 3 classes)

### **Production Features:**
- ğŸš€ **FastAPI Server**: Real-time inference with A/B testing
- ğŸ“Š **Monitoring System**: Automatic drift detection with escalating validation
- ğŸ”„ **Auto-Retraining**: Triggered retraining when performance degrades
- ğŸ“„ **PDF Report Generation**: Professional reports using ReportLab
- ğŸ—„ï¸ **PostgreSQL Integration**: Production data management
- â˜ï¸ **AWS Integration**: Secrets Manager support

### **Research & Evaluation Features (Phase 2):**
- ğŸ”¬ **5-Fold Cross-Validation**: Stratified k-fold CV with held-out test set
- ğŸ“Š **Statistical Hypothesis Testing**: Chi-square tests for class balance analysis
- ğŸ” **Comprehensive Error Analysis**: 7 detailed analysis modules
  - Confusion matrix deep dive
  - Per-class performance breakdown
  - Confidence analysis (correct vs incorrect predictions)
  - Input length analysis (accuracy by token length)
  - Failure case extraction (top 50 most confident mistakes)
  - Token-level attribution (attention-based)
  - Jailbreak-specific error analysis
- ğŸ“ˆ **Publication-Ready Results**: Mean Â± std metrics across folds with statistical rigor

---

## ğŸ“ Project Structure

```
3-Class-Refusal-Classifier-with-RoBERTa/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 01-Imports.py              # Central import manager
â”‚   â”œâ”€â”€ 02-Config.py                # All configuration settings
â”‚   â”œâ”€â”€ 03-Constants.py             # Global constants
â”‚   â”œâ”€â”€ 04-AWSConfig.py             # AWS configuration
â”‚   â”œâ”€â”€ 05-SecretsHandler.py        # AWS Secrets Manager
â”‚   â”œâ”€â”€ 06-PromptGenerator.py       # 3-stage prompt generation
â”‚   â”œâ”€â”€ 07-ResponseCollector.py     # Multi-LLM response collection
â”‚   â”œâ”€â”€ 08-DataCleaner.py           # Comprehensive data cleaning
â”‚   â”œâ”€â”€ 09-DataLabeler.py           # LLM judge labeling
â”‚   â”œâ”€â”€ 10-LabelingQualityAnalyzer.py
â”‚   â”œâ”€â”€ 11-ClassificationDataset.py # PyTorch Dataset
â”‚   â”œâ”€â”€ 12-RefusalClassifier.py     # 3-class RoBERTa model
â”‚   â”œâ”€â”€ 13-JailbreakDetector.py     # 2-class RoBERTa model
â”‚   â”œâ”€â”€ 14-Trainer.py               # Generic trainer with weighted loss
â”‚   â”œâ”€â”€ 15-PerModelAnalyzer.py      # Per-model performance analysis
â”‚   â”œâ”€â”€ 16-ConfidenceAnalyzer.py    # Confidence score analysis
â”‚   â”œâ”€â”€ 17-AdversarialTester.py     # Paraphrasing robustness tests
â”‚   â”œâ”€â”€ 18-JailbreakAnalysis.py     # Security analysis
â”‚   â”œâ”€â”€ 19-AttentionVisualizer.py   # Attention heatmaps
â”‚   â”œâ”€â”€ 20-ShapAnalyzer.py          # SHAP interpretability
â”‚   â”œâ”€â”€ 21-PowerLawAnalyzer.py      # Power law analysis
â”‚   â”œâ”€â”€ 22-Visualizer.py            # Basic plotting functions
â”‚   â”œâ”€â”€ 23-ReportGenerator.py       # PDF report generation
â”‚   â”œâ”€â”€ 24-RefusalPipeline.py       # Main training pipeline
â”‚   â”œâ”€â”€ 25-ExperimentRunner.py      # Experiment orchestration
â”‚   â”œâ”€â”€ 26-Execute.py               # Main entry point
â”‚   â”œâ”€â”€ 27-Analyze.py               # Analysis script
â”‚   â”œâ”€â”€ 28-ProductionAPI.py         # FastAPI server
â”‚   â”œâ”€â”€ 29-MonitoringSystem.py      # Production monitoring
â”‚   â”œâ”€â”€ 30-RetrainingPipeline.py    # Automated retraining
â”‚   â”œâ”€â”€ 31-DataManager.py           # Production data management
â”‚   â”œâ”€â”€ 32-CrossValidator.py        # K-fold cross-validation (Phase 2)
â”‚   â”œâ”€â”€ 33-HypothesisTesting.py     # Statistical hypothesis tests (Phase 2)
â”‚   â””â”€â”€ 34-ErrorAnalysis.py         # Comprehensive error analysis (Phase 2)
â”œâ”€â”€ data/                           # Created automatically
â”œâ”€â”€ models/                         # Created automatically
â”œâ”€â”€ results/                        # Created automatically
â”œâ”€â”€ visualizations/                 # Created automatically
â”œâ”€â”€ reports/                        # Created automatically
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ .gitignore                      # Git ignore patterns
â””â”€â”€ README.md                       # This file
```

---

## ğŸš€ Quick Start

### **1. Installation**

```bash
# Clone repository
git clone https://github.com/yourusername/3-Class-Refusal-Classifier-with-RoBERTa.git
cd 3-Class-Refusal-Classifier-with-RoBERTa

# Install dependencies
pip install -r requirements.txt
```

### **2. Set API Keys**

```bash
# Required API keys
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"

# Optional (for AWS features)
export AWS_ACCESS_KEY_ID="your-aws-key"
export AWS_SECRET_ACCESS_KEY="your-aws-secret"
```

### **3. Run Quick Test**

```bash
python src/26-Execute.py --test
```

### **4. Run Full Experiment**

```bash
# Interactive mode
python src/26-Execute.py

# CLI modes
python src/26-Execute.py --full         # Full pipeline
python src/26-Execute.py --train-only   # Training only
python src/26-Execute.py --analyze-only # Analysis only
python src/26-Execute.py --cv           # Cross-validation mode (Phase 2)
python src/26-Execute.py --cv 10        # Cross-validation with 10 folds
```

---

## ğŸ“Š Usage Examples

### **Training & Analysis:**

```bash
# Full experiment with all stages
python src/26-Execute.py --full

# Cross-validation with hypothesis testing and error analysis (Phase 2)
python src/26-Execute.py --cv           # 5-fold CV (default)
python src/26-Execute.py --cv 10        # 10-fold CV

# Analyze existing models with PDF reports
python src/27-Analyze.py --auto --generate-report

# Specify custom models
python src/27-Analyze.py --refusal-model models/my_model.pt \
                         --jailbreak-model models/jailbreak.pt \
                         --generate-report --report-type performance
```

### **Production API:**

```bash
# Start production server
python src/28-ProductionAPI.py

# Test classification endpoint
curl -X POST http://localhost:8000/classify \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "How do I hack a website?",
    "response": "I cannot help with that request."
  }'
```

---

## ğŸ—ï¸ Architecture

### **Data Pipeline:**
```
Prompt Generation (3-stage) â†’ Multi-LLM Response Collection â†’ Data Cleaning â†’ LLM Judge Labeling â†’ Quality Analysis â†’ Train/Val/Test Split
```

### **Training Pipeline:**
```
Load Data â†’ Create Dataset â†’ Initialize Model (RoBERTa) â†’ Weighted Loss â†’ Train with Early Stopping â†’ Validation â†’ Save Best Model
```

### **Analysis Pipeline:**
```
Load Model â†’ Per-Model Analysis â†’ Confidence Analysis â†’ Adversarial Testing â†’ Jailbreak Analysis â†’ SHAP â†’ Attention â†’ Power Law â†’ Visualizations â†’ PDF Reports
```

### **Production Pipeline:**
```
FastAPI Server â†’ Model Inference â†’ Log to PostgreSQL â†’ Monitor Performance â†’ Detect Drift â†’ Trigger Retraining â†’ A/B Test â†’ Deploy
```

---

## ğŸ“ˆ Model Performance

**Refusal Classifier (3-class):**
- Classes: No Refusal, Hard Refusal, Soft Refusal
- Model: RoBERTa-base fine-tuned
- Training: Weighted CrossEntropyLoss (class imbalance handling)
- Evaluation: F1-score (macro), Precision, Recall, Accuracy

**Jailbreak Detector (2-class):**
- Classes: Attack Failed, Attack Succeeded
- Model: RoBERTa-base fine-tuned
- Focus: Security-critical detection

*(Run experiments to get specific metrics)*

---

## ğŸ”¬ Interpretability

The project includes comprehensive interpretability tools:

1. **SHAP Analysis**: Token-level feature importance
2. **Attention Visualization**: Multi-head attention heatmaps
3. **Power Law Analysis**: Pareto principle in predictions
4. **Confidence Analysis**: Calibration and uncertainty
5. **Adversarial Robustness**: Paraphrasing sensitivity

---

## ğŸ“¦ Configuration

All settings are centralized in `src/02-Config.py`:

- **API Configuration**: Models, rate limits, retries
- **Dataset Configuration**: Sample sizes, categories
- **Training Configuration**: Epochs, batch size, learning rate
- **Model Configuration**: Architecture, num_classes, dropout
- **Production Configuration**: Monitoring, A/B testing, retraining

---

## ğŸ§ª Testing

```bash
# Quick test (reduced samples)
python src/26-Execute.py --test

# Train only (uses existing data)
python src/26-Execute.py --train-only

# Analyze only (uses existing models)
python src/26-Execute.py --analyze-only
```

---

## ğŸ“„ Reports

Generate professional PDF reports:

```bash
# All reports (performance + interpretability + executive summary)
python src/27-Analyze.py --auto --generate-report --report-type all

# Performance report only
python src/27-Analyze.py --auto --generate-report --report-type performance

# Executive summary only
python src/27-Analyze.py --auto --generate-report --report-type executive
```

Reports are saved to `reports/` directory.

---

## ğŸ” Security

**Important:** Never commit API keys or sensitive data!

- All API keys via environment variables
- AWS Secrets Manager integration available
- `.gitignore` configured to exclude secrets
- Production API requires admin key configuration

---

## ğŸ› ï¸ Development

### **File Numbering Convention:**
Files are numbered 01-31 for clear execution order. Auto-loaded files: 04-25.

### **Generic Design:**
All analyzers and visualizers use `class_names` parameter - works with any N-class classifier.

### **Adding New Features:**
1. Add module to `src/`
2. Follow naming convention: `XX-ModuleName.py`
3. Include header: `# All imports are in 01-Imports.py`
4. Update `01-Imports.py` if needed

---

## ğŸ“š Dependencies

See `requirements.txt` for full list. Key dependencies:

- **PyTorch** (2.0+): Deep learning framework
- **Transformers** (4.30+): Hugging Face models
- **FastAPI** (0.100+): Production API
- **ReportLab** (4.0+): PDF generation
- **SHAP** (0.42+): Interpretability

---

## ğŸ¤ Contributing

This is a research/production project. For questions or suggestions, please open an issue.

---

## ğŸ“ License

[Specify your license - see LICENSE file]

---

## ğŸ‘¤ Author

Ramy Alsaffar

---

## ğŸ™ Acknowledgments

- **RoBERTa**: Liu et al., 2019
- **SHAP**: Lundberg & Lee, 2017
- **Transformers**: Hugging Face team
- **FastAPI**: SebastiÃ¡n RamÃ­rez

---

## ğŸ“§ Contact

[Your contact information]

---

**Last Updated:** October 31, 2025
