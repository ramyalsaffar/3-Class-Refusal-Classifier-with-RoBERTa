# Dual RoBERTa Classifiers: 3-Class Refusal Taxonomy & Binary Jailbreak Detection

A production-ready, dual-task classification system using fine-tuned RoBERTa models for comprehensive LLM safety analysis:
1. **Refusal Classification** (3 classes): No Refusal, Hard Refusal, Soft Refusal
2. **Jailbreak Detection** (2 classes): Attack Failed, Attack Succeeded

---

## ğŸ§  Why RoBERTa?

**RoBERTa** (Robustly Optimized BERT Pretraining Approach) was selected as the backbone model based on extensive literature demonstrating its superiority for text classification tasks, particularly in safety-critical domains:

### **Literature Support:**

1. **Superior Text Classification Performance** (Liu et al., 2019)
   - RoBERTa achieves state-of-the-art results on GLUE, RACE, and SQuAD benchmarks
   - Outperforms BERT through: dynamic masking, larger batch sizes, removal of Next Sentence Prediction (NSP)
   - Trained on 160GB of text (10x more than BERT) with longer sequences

2. **Robust for Safety & Toxicity Detection** (Vidgen et al., 2021; Pozzobon et al., 2023)
   - RoBERTa-based models consistently outperform alternatives on hate speech and toxic content detection
   - Strong performance on nuanced classification tasks requiring contextual understanding

3. **Effective for Refusal Pattern Recognition** (Qi et al., 2023; RÃ¶ttger et al., 2024)
   - Transformer models like RoBERTa excel at capturing linguistic patterns in LLM safety behaviors
   - Bidirectional attention mechanism critical for understanding subtle refusal cues ("soft refusals")

4. **Production-Ready & Well-Supported**
   - Extensive Hugging Face ecosystem with 12,000+ RoBERTa checkpoints
   - Efficient fine-tuning with minimal compute requirements
   - Proven deployment at scale (OpenAI, Google, Meta)

### **Why Not Baseline Comparisons?**

**No baseline analysis was conducted** as the literature already establishes RoBERTa as the optimal choice for:
- Multi-class text classification
- Contextual understanding of nuanced language
- Safety-critical NLP applications
- Production deployment constraints

**Key References:**
- Liu, Y., et al. (2019). "RoBERTa: A Robustly Optimized BERT Pretraining Approach." *arXiv:1907.11692*
- Vidgen, B., et al. (2021). "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection." *ACL 2021*
- Qi, X., et al. (2023). "Fine-tuning Aligned Language Models Compromises Safety." *ICLR 2024*

---

## ğŸ¯ Features

### **Core Capabilities:**
- âœ… **Dual-Task Classification**: Simultaneously detects refusal patterns AND jailbreak success
- âœ… **Production-Ready**: Full API server with A/B testing, monitoring, and auto-retraining
- âœ… **Comprehensive Interpretability**: SHAP analysis, attention visualization, power law analysis
- âœ… **LLM Judge Labeling**: GPT-4o-based labeling with confidence scores and randomized class ordering
- âœ… **3-Stage Prompt Generation**: Quality-controlled prompt generation pipeline
- âœ… **Multi-Model Data Collection**: Claude Sonnet 4.5, GPT-5, Gemini 2.5 Flash
- âœ… **Generic Design**: Works with any N-class classifier (not hardcoded to 3 classes)

### **Production Features:**
- ğŸš€ **FastAPI Server**: Real-time inference with A/B testing
- ğŸ“Š **Monitoring System**: Automatic drift detection with escalating validation
- ğŸ”„ **Auto-Retraining**: Triggered retraining when performance degrades
- ğŸ“„ **PDF Report Generation**: Professional reports using ReportLab
- ğŸ—„ï¸ **PostgreSQL Integration**: Production data management
- â˜ï¸ **AWS Integration**: Secrets Manager support
- ğŸ³ **Docker & Containerization**: Multi-stage builds with GPU support for easy deployment

### **Research & Evaluation Features (Phase 2):**
- ğŸ”¬ **5-Fold Cross-Validation**: Stratified k-fold CV with held-out test set
- ğŸ“Š **Statistical Hypothesis Testing**: Chi-square tests for class balance analysis
- ğŸ” **Comprehensive Error Analysis**: 7 detailed analysis modules
  - Confusion matrix deep dive
  - Per-class performance breakdown
  - Confidence analysis (correct vs incorrect predictions)
  - Input length analysis (accuracy by token length)
  - Failure case extraction (top 50 most confident mistakes)
  - Token-level attribution (attention-based)
  - Jailbreak-specific error analysis
- ğŸ“ˆ **Publication-Ready Results**: Mean Â± std metrics across folds with statistical rigor

---

## ğŸ“ Project Structure

```
Dual-RoBERTa-Classifiers/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 01-Imports.py              # Central import manager
â”‚   â”œâ”€â”€ 02-Config.py                # All configuration settings (includes AWS config)
â”‚   â”œâ”€â”€ 03-Constants.py             # Global constants
â”‚   â”œâ”€â”€ 04-SecretsHandler.py        # AWS Secrets Manager
â”‚   â”œâ”€â”€ 05-PromptGenerator.py       # 3-stage prompt generation
â”‚   â”œâ”€â”€ 06-ResponseCollector.py     # Multi-LLM response collection
â”‚   â”œâ”€â”€ 07-DataCleaner.py           # Comprehensive data cleaning
â”‚   â”œâ”€â”€ 08-DataLabeler.py           # LLM judge labeling
â”‚   â”œâ”€â”€ 09-LabelingQualityAnalyzer.py
â”‚   â”œâ”€â”€ 10-ClassificationDataset.py # PyTorch Dataset
â”‚   â”œâ”€â”€ 11-RefusalClassifier.py     # 3-class RoBERTa model
â”‚   â”œâ”€â”€ 12-JailbreakDetector.py     # 2-class RoBERTa model
â”‚   â”œâ”€â”€ 13-Trainer.py               # Standard trainer with weighted loss
â”‚   â”œâ”€â”€ 14-CrossValidator.py        # K-fold cross-validation (Phase 2)
â”‚   â”œâ”€â”€ 15-PerModelAnalyzer.py      # Per-model performance analysis
â”‚   â”œâ”€â”€ 16-ConfidenceAnalyzer.py    # Confidence score analysis
â”‚   â”œâ”€â”€ 17-AdversarialTester.py     # Paraphrasing robustness tests
â”‚   â”œâ”€â”€ 18-JailbreakAnalysis.py     # Security-focused jailbreak analysis
â”‚   â”œâ”€â”€ 19-CorrelationAnalysis.py   # Refusal â†” Jailbreak correlation (Phase 2)
â”‚   â”œâ”€â”€ 20-AttentionVisualizer.py   # Attention heatmaps
â”‚   â”œâ”€â”€ 21-ShapAnalyzer.py          # SHAP interpretability
â”‚   â”œâ”€â”€ 22-PowerLawAnalyzer.py      # Power law analysis
â”‚   â”œâ”€â”€ 23-HypothesisTesting.py     # Statistical hypothesis tests (Phase 2)
â”‚   â”œâ”€â”€ 24-ErrorAnalysis.py         # Comprehensive error analysis (Phase 2)
â”‚   â”œâ”€â”€ 25-Visualizer.py            # Basic plotting functions
â”‚   â”œâ”€â”€ 26-ReportGenerator.py       # PDF report generation
â”‚   â”œâ”€â”€ 27-RefusalPipeline.py       # Main training pipeline
â”‚   â”œâ”€â”€ 28-ExperimentRunner.py      # Experiment orchestration
â”‚   â”œâ”€â”€ 29-Execute.py               # Main entry point
â”‚   â”œâ”€â”€ 30-Analyze.py               # Analysis script
â”‚   â”œâ”€â”€ 31-ProductionAPI.py         # FastAPI server
â”‚   â”œâ”€â”€ 32-MonitoringSystem.py      # Production monitoring
â”‚   â”œâ”€â”€ 33-RetrainingPipeline.py    # Automated retraining
â”‚   â””â”€â”€ 34-DataManager.py           # Production data management
â”œâ”€â”€ data/                           # Created automatically
â”œâ”€â”€ models/                         # Created automatically
â”œâ”€â”€ results/                        # Created automatically
â”œâ”€â”€ visualizations/                 # Created automatically
â”œâ”€â”€ reports/                        # Created automatically
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ Dockerfile                      # Docker multi-stage build
â”œâ”€â”€ docker-compose.yml              # Docker Compose configuration
â”œâ”€â”€ .dockerignore                   # Docker ignore patterns
â”œâ”€â”€ .gitignore                      # Git ignore patterns
â””â”€â”€ README.md                       # This file
```

---

## ğŸš€ Quick Start

### **1. Installation**

```bash
# Clone repository
git clone https://github.com/yourusername/Dual-RoBERTa-Classifiers.git
cd Dual-RoBERTa-Classifiers

# Install dependencies
pip install -r requirements.txt
```

### **2. Set API Keys**

```bash
# Required API keys
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"

# Optional (for AWS features)
export AWS_ACCESS_KEY_ID="your-aws-key"
export AWS_SECRET_ACCESS_KEY="your-aws-secret"
```

### **3. Run Quick Test**

```bash
python src/29-Execute.py --test
```

### **4. Run Full Experiment**

```bash
# Interactive mode
python src/29-Execute.py

# CLI modes
python src/29-Execute.py --full         # Full pipeline
python src/29-Execute.py --train-only   # Training only
python src/29-Execute.py --analyze-only # Analysis only
python src/29-Execute.py --cv           # Cross-validation mode (Phase 2)
python src/29-Execute.py --cv 10        # Cross-validation with 10 folds
```

---

## ğŸ³ Docker Deployment

### **Why Docker?**
Docker provides environment consistency, easy deployment, and reproducibility across different machines and environments.

### **Quick Start with Docker:**

```bash
# 1. Build the image
docker-compose build dev

# 2. Start development environment
docker-compose up dev

# Or use specific services:
docker-compose up train      # Full training pipeline
docker-compose up train-cv   # Cross-validation (Phase 2)
docker-compose up analyze    # Analysis only
docker-compose up api        # Production API server
docker-compose up jupyter    # Jupyter notebook
```

### **Docker Services Available:**

| Service | Purpose | Command |
|---------|---------|---------|
| `dev` | Interactive development | `docker-compose up dev` |
| `train` | Full training pipeline | `docker-compose up train` |
| `train-cv` | Cross-validation training | `docker-compose up train-cv` |
| `analyze` | Analysis with reports | `docker-compose up analyze` |
| `api` | Production API server | `docker-compose up api` |
| `jupyter` | Jupyter notebook | `docker-compose up jupyter` |

### **Environment Setup:**

Create a `.env` file in the project root with your API keys:

```bash
# .env file
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key

# Optional AWS credentials
AWS_ACCESS_KEY_ID=your-aws-key
AWS_SECRET_ACCESS_KEY=your-aws-secret
```

### **GPU Support:**

Docker Compose is configured for GPU support. To use CPU-only:

```bash
# Comment out the deploy section in docker-compose.yml
# Or use CPU-only base image
docker build --target production -t refusal-classifier:cpu .
```

### **Common Docker Commands:**

```bash
# Build all images
docker-compose build

# Run full training pipeline (with GPU)
docker-compose up train

# Run cross-validation (Phase 2)
docker-compose up train-cv

# Start API server
docker-compose up -d api

# Access development container shell
docker-compose run --rm dev /bin/bash

# View API logs
docker-compose logs -f api

# Stop all services
docker-compose down

# Remove all containers and volumes
docker-compose down -v
```

### **Volume Mounts:**

Docker containers mount local directories for persistence:

- `./data` â†’ `/app/data` - Training data
- `./models` â†’ `/app/models` - Trained models
- `./results` â†’ `/app/results` - Analysis results
- `./visualizations` â†’ `/app/visualizations` - Plots
- `./reports` â†’ `/app/reports` - PDF reports

**WHY:** Changes persist even when containers are stopped/restarted.

### **Production Deployment:**

```bash
# Build production API image
docker build --target api -t refusal-classifier:api .

# Run with Docker
docker run -d \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models:ro \
  --name refusal-api \
  refusal-classifier:api

# Or use Docker Compose
docker-compose up -d api

# Health check
curl http://localhost:8000/health
```

---

## ğŸ“Š Usage Examples

### **Training & Analysis:**

```bash
# Full experiment with all stages
python src/29-Execute.py --full

# Cross-validation with hypothesis testing and error analysis (Phase 2)
python src/29-Execute.py --cv           # 5-fold CV (default)
python src/29-Execute.py --cv 10        # 10-fold CV

# Analyze existing models with PDF reports
python src/30-Analyze.py --auto --generate-report

# Specify custom models
python src/30-Analyze.py --refusal-model models/my_model.pt \
                         --jailbreak-model models/jailbreak.pt \
                         --generate-report --report-type performance
```

### **Production API:**

```bash
# Start production server
python src/31-ProductionAPI.py

# Test classification endpoint
curl -X POST http://localhost:8000/classify \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "How do I hack a website?",
    "response": "I cannot help with that request."
  }'
```

---

## ğŸ—ï¸ Architecture

### **Data Pipeline:**
```
Prompt Generation (3-stage) â†’ Multi-LLM Response Collection â†’ Data Cleaning â†’ LLM Judge Labeling â†’ Quality Analysis â†’ Train/Val/Test Split
```

### **Training Pipeline:**
```
Load Data â†’ Create Dataset â†’ Initialize Model (RoBERTa) â†’ Weighted Loss â†’ Train with Early Stopping â†’ Validation â†’ Save Best Model
```

### **Analysis Pipeline:**
```
Load Model â†’ Per-Model Analysis â†’ Confidence Analysis â†’ Adversarial Testing â†’ Jailbreak Analysis â†’ SHAP â†’ Attention â†’ Power Law â†’ Visualizations â†’ PDF Reports
```

### **Production Pipeline:**
```
FastAPI Server â†’ Model Inference â†’ Log to PostgreSQL â†’ Monitor Performance â†’ Detect Drift â†’ Trigger Retraining â†’ A/B Test â†’ Deploy
```

---

## ğŸ“ˆ Model Performance

**Refusal Classifier (3-class):**
- Classes: No Refusal, Hard Refusal, Soft Refusal
- Model: RoBERTa-base fine-tuned
- Training: Weighted CrossEntropyLoss (class imbalance handling)
- Evaluation: F1-score (macro), Precision, Recall, Accuracy

**Jailbreak Detector (2-class):**
- Classes: Attack Failed, Attack Succeeded
- Model: RoBERTa-base fine-tuned
- Focus: Security-critical detection

*(Run experiments to get specific metrics)*

---

## ğŸ”¬ Interpretability

The project includes comprehensive interpretability tools:

1. **SHAP Analysis**: Token-level feature importance
2. **Attention Visualization**: Multi-head attention heatmaps
3. **Power Law Analysis**: Pareto principle in predictions
4. **Confidence Analysis**: Calibration and uncertainty
5. **Adversarial Robustness**: Paraphrasing sensitivity

---

## ğŸ“¦ Configuration

All settings are centralized in `src/02-Config.py`:

- **API Configuration**: Models, rate limits, retries
- **Dataset Configuration**: Sample sizes, categories
- **Training Configuration**: Epochs, batch size, learning rate
- **Model Configuration**: Architecture, num_classes, dropout
- **Production Configuration**: Monitoring, A/B testing, retraining

---

## ğŸ§ª Testing

```bash
# Quick test (reduced samples)
python src/29-Execute.py --test

# Train only (uses existing data)
python src/29-Execute.py --train-only

# Analyze only (uses existing models)
python src/29-Execute.py --analyze-only
```

---

## ğŸ“„ Reports

Generate professional PDF reports:

```bash
# All reports (performance + interpretability + executive summary)
python src/30-Analyze.py --auto --generate-report --report-type all

# Performance report only
python src/30-Analyze.py --auto --generate-report --report-type performance

# Executive summary only
python src/30-Analyze.py --auto --generate-report --report-type executive
```

Reports are saved to `reports/` directory.

---

## ğŸ” Security

**Important:** Never commit API keys or sensitive data!

- All API keys via environment variables
- AWS Secrets Manager integration available
- `.gitignore` configured to exclude secrets
- Production API requires admin key configuration

---

## ğŸ› ï¸ Development

### **File Numbering Convention:**
Files are numbered 01-31 for clear execution order. Auto-loaded files: 04-25.

### **Generic Design:**
All analyzers and visualizers use `class_names` parameter - works with any N-class classifier.

### **Adding New Features:**
1. Add module to `src/`
2. Follow naming convention: `XX-ModuleName.py`
3. Include header: `# All imports are in 01-Imports.py`
4. Update `01-Imports.py` if needed

---

## ğŸ“š Dependencies

See `requirements.txt` for full list. Key dependencies:

- **PyTorch** (2.0+): Deep learning framework
- **Transformers** (4.30+): Hugging Face models
- **FastAPI** (0.100+): Production API
- **ReportLab** (4.0+): PDF generation
- **SHAP** (0.42+): Interpretability

---

## ğŸ¤ Contributing

This is a research/production project. For questions or suggestions, please open an issue.

---

## ğŸ“ License

[Specify your license - see LICENSE file]

---

## ğŸ‘¤ Author

Ramy Alsaffar

---

## ğŸ™ Acknowledgments

- **RoBERTa**: Liu et al., 2019
- **SHAP**: Lundberg & Lee, 2017
- **Transformers**: Hugging Face team
- **FastAPI**: SebastiÃ¡n RamÃ­rez

---

## ğŸ“§ Contact

[Your contact information]

---

**Last Updated:** November 3, 2025
